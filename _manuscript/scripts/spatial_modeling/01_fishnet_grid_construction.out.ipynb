{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Point to grid conversion\n",
        "\n",
        "This code takes the georeferenced points and related them to a fishnet\n",
        "grid of 1km x 1km scale.\n",
        "\n",
        "This will be for both for Australian Plague Locust and Desert Locust"
      ],
      "id": "e5eee53e-70d6-48d0-bb5b-d27df71d3fac"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import os"
      ],
      "id": "8cb97841-63d4-4070-a986-089de1d711e8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'/home/datascience/herbivore_nutrient_interactions'"
            ]
          }
        }
      ],
      "source": [
        "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
        "os.getcwd()"
      ],
      "id": "1184a69a-588f-4522-bd5a-04540769e463"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function takes the point dataset and creates a sparse fishnet grid that overlaps with all points. \n",
        "# there is a user species cell size and few other handy parameters\n",
        "# it defaults to parallel processing but can be turned off\n",
        "\n",
        "# it creates a bounding box, creates a coarse grid, filters out unused grids and then within each coarse grid, it creates the fine scale grid\n",
        "# at the speicifed cell size and then removes unused grids.\n",
        "\n",
        "with open('scripts/functions/fishnet_grid_function.py') as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Execute the code\n",
        "exec(code)\n"
      ],
      "id": "20638780-1e5a-4d63-b733-5b41a2670c7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Australian plague locust data\n",
        "\n",
        "This dataset is comprised of several species but the records are biased\n",
        "towards the more economically important species (e.g. australian plauge\n",
        "locust)\n",
        "\n",
        "## Species identifier\n",
        "\n",
        "The `Species` column is the identifier per species\n",
        "\n",
        "-   10 = no locust/grasshopper found\n",
        "-   11 = Australian plague locust (*Chortocietes terminifera*)\n",
        "-   12 = Spur-throated locust (*Austracris guttulosa*)\n",
        "-   15 = Small plague locust (*Austroicetes cruciata*)\n",
        "-   19 = Eastern plague grasshopper (*Oedaleus australis*)\n",
        "\n",
        "### Species data management\n",
        "\n",
        "For each species, we need to combined the species specific ID with the\n",
        "`10` observations (nil-observations) and observations in the other\n",
        "species categories that are nil. This will provide us with all the\n",
        "records that a specific grasshopper species was present and combine it\n",
        "with all the nil observation possible."
      ],
      "id": "3692112a-03e4-4aaa-8a5a-87f1c2b78c1d"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/raw/survey_data/APLC_surveys_2000-2017.csv')"
      ],
      "id": "19398ba1-1ad5-44d4-a42c-c26ee37c0a52"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_and_update_species(df, species_list, target_species):\n",
        "    and_nil = df['Species'].isin(species_list)\n",
        "    all_other_nil = (\n",
        "        ~df['Species'].isin(species_list) &\n",
        "        (df['Nymph Density'] == 0) &\n",
        "        (df['Adult Density'] == 0)\n",
        "    )\n",
        "    filtered_df = df[and_nil | all_other_nil].copy()  # Use copy() to avoid SettingWithCopyWarning\n",
        "    filtered_df['Species'] = target_species\n",
        "    return filtered_df\n",
        "\n",
        "# Define the list of target species and corresponding species lists\n",
        "species_combinations = [\n",
        "    ([10, 11], 11),\n",
        "    ([10, 12], 12),\n",
        "    ([10, 15], 15),\n",
        "    ([10, 19], 19)\n",
        "]\n",
        "\n",
        "# Use a list comprehension to generate all DataFrames in one step\n",
        "full_species_data = pd.concat(\n",
        "    [filter_and_update_species(df, species_list, target_species) \n",
        "     for species_list, target_species in species_combinations], \n",
        "    ignore_index=True\n",
        ").drop_duplicates()  # Optionally drop duplicates\n",
        "\n",
        "# final_df now contains the combined result\n",
        "full_species_data.head()"
      ],
      "id": "f22158aa-4a9b-4b5f-a743-c92efb33e85e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fishnet grid construction\n",
        "\n",
        "I first create the sparse fishnet grid (roughly 1km x 1km) and then\n",
        "aggregate the species data to the polygons. This is done BY species and\n",
        "not between."
      ],
      "id": "9adf94b1-4cca-44fc-91dc-c205b409e44f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Polygons: 100%|██████████| 1315/1315 [00:13<00:00, 94.54it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "aplc_survey_dataframe = create_grids_parallel(\n",
        "        full_species_data,\n",
        "        longitude_column='Longitude',\n",
        "        latitude_column='Latitude',\n",
        "        csv_crs='EPSG:4326',\n",
        "        grid_crs='EPSG:4326',\n",
        "        transformation_crs='EPSG:4326',\n",
        "        final_cell_size=0.01,\n",
        "        coarse_cell_size=None,\n",
        "        parallel=True\n",
        "    )\n",
        "\n"
      ],
      "id": "b518e127-c4da-4ed5-a189-161cb686a630"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>67176 rows × 2 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Add an index to the polygons\n",
        "## this will allow us to join tables together (and make GEE easier)\n",
        "\n",
        "aplc_survey_dataframe = gpd.GeoDataFrame(aplc_survey_dataframe)\n",
        "aplc_survey_dataframe['polygon_id'] = aplc_survey_dataframe.index\n",
        "aplc_survey_dataframe.set_crs(epsg=4326, inplace=True)"
      ],
      "id": "cd8ea9d3-c931-4001-81c5-983391ccfa35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lets summarize the APL point data to the polygon grid data"
      ],
      "id": "087fd2b5-667b-4b08-8a6b-f385a6a6e757"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_keep = ['Longitude', 'Latitude', 'Date','Species','Data Quality','Nymph Density','Adult Density']\n",
        "APL_dat = full_species_data[columns_to_keep]\n"
      ],
      "id": "674d1a74-aa45-4e26-a3b1-335111a9e054"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>309640 rows × 8 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Convert DataFrame to GeoDataFrame\n",
        "\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    APL_dat,\n",
        "    geometry=gpd.points_from_xy(APL_dat.Longitude, APL_dat.Latitude)\n",
        ")\n",
        "\n",
        "gdf_points.set_crs(epsg=4326, inplace=True)\n"
      ],
      "id": "b13906f9-6919-4226-ad4b-d9788b6036c3"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "joined_gdf = gpd.sjoin(gdf_points, aplc_survey_dataframe, how=\"left\", predicate='within')\n"
      ],
      "id": "b979461e-3cec-4b0d-8fe6-2b6a058a46b6"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitude        0\n",
            "Latitude         0\n",
            "Date             0\n",
            "Species          0\n",
            "Data Quality     0\n",
            "Nymph Density    0\n",
            "Adult Density    0\n",
            "geometry         0\n",
            "index_right      0\n",
            "polygon_id       0\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "# I wanted to ensure every point has an assoicated polygon_id \n",
        "\n",
        "## which they do\n",
        "\n",
        "na_counts = joined_gdf.isna().sum()\n",
        "\n",
        "print(na_counts)"
      ],
      "id": "4982db06-9b18-401d-99a2-8e4fb78a5a80"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom aggregation function to get min, median, and max dates\n",
        "def date_aggregations(series):\n",
        "    series = pd.to_datetime(series, format='%d-%b-%y') # converts to a datatime\n",
        "    return [series.min(), series.median(), series.max()]\n",
        "\n",
        "# aggregation function to count unique values\n",
        "def count_unique_values(series):\n",
        "    return series.value_counts().to_dict()\n",
        "\n",
        "# Group by polygon geometry and aggregate the data\n",
        "aggregated_gdf = joined_gdf.groupby(['Species', 'polygon_id']).agg({\n",
        "    'Date': date_aggregations,\n",
        "    'Longitude': 'median',\n",
        "    'Latitude': 'median',\n",
        "    'Data Quality': count_unique_values,\n",
        "    'Nymph Density': count_unique_values,\n",
        "    'Adult Density': count_unique_values\n",
        "}).reset_index()\n"
      ],
      "id": "154aac4f-b185-43d6-9897-22eec2641829"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>177613 rows × 8 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "aggregated_gdf"
      ],
      "id": "a1aecc29-106a-4d7b-bd83-c06e07c84cc5"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Lets count up the total number of observations in the dataframe:\n",
        "\n",
        "# Define the aggregation functions for each column\n",
        "agg_funcs = {\n",
        "    'Data Quality': 'count',  \n",
        "    'Adult Density': 'count',    \n",
        "    'Nymph Density': 'count'    \n",
        "}\n",
        "\n",
        "# Perform groupby operation and aggregate\n",
        "grouped_joined_gdf = joined_gdf.groupby(['Species', 'polygon_id']).agg(agg_funcs).reset_index()\n",
        "\n",
        "# Rename the aggregated columns\n",
        "grouped_joined_gdf.rename(columns={\n",
        "    'Data Quality': 'Data Quality Total Count',\n",
        "    'Adult Density': 'Adult Density Total Count',\n",
        "    'Nymph Density': 'Nymph Density Total Count'\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "# These three columns should agree with one another....\n",
        "## if any rows dont -- this command filters for them\n",
        "grouped_joined_gdf[\n",
        "    (grouped_joined_gdf['Data Quality Total Count'] != grouped_joined_gdf['Adult Density Total Count']) |\n",
        "    (grouped_joined_gdf['Data Quality Total Count'] != grouped_joined_gdf['Nymph Density Total Count']) |\n",
        "    (grouped_joined_gdf['Adult Density Total Count'] != grouped_joined_gdf['Nymph Density Total Count'])\n",
        "]"
      ],
      "id": "220eaf11-46b0-46e3-a435-af9fcf1fdefc"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Species                      0\n",
            "polygon_id                   0\n",
            "Date                         0\n",
            "Longitude                    0\n",
            "Latitude                     0\n",
            "Data Quality                 0\n",
            "Nymph Density                0\n",
            "Adult Density                0\n",
            "Data Quality Total Count     0\n",
            "Adult Density Total Count    0\n",
            "Nymph Density Total Count    0\n",
            "geometry                     0\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "final_df = pd.merge(aggregated_gdf, grouped_joined_gdf, on=['Species', 'polygon_id'], how='left')\n",
        "final_df = pd.merge(final_df, aplc_survey_dataframe, on='polygon_id', how='left')\n",
        "final_df.head()\n",
        "na_counts = final_df.isna().sum()\n",
        "\n",
        "print(na_counts)"
      ],
      "id": "66a01f23-45b0-41f3-8efa-c2f0244c59fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The APLC data is complete – lets write to disk"
      ],
      "id": "7d8b5a28-0e39-47aa-9967-fc219ada0ee9"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv('data/processed/spatial_modeling/aplc_data_aggregated_to_polygon_grid.csv')"
      ],
      "id": "53baf12a-4255-4c56-997e-b89028f78bac"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}