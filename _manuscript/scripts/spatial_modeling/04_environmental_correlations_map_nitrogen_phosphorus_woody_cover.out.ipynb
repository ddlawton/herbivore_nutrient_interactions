{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environmental variable correlations\n",
        "\n",
        "This script extracts nitrogen, phosphorus, woody vegetation pixel\n",
        "coverage, and mean annual precipitation from google earth engine for\n",
        "correlation\n",
        "\n",
        "The purpose is to show how the locust outbreak model disentangled the\n",
        "positive correlation between nitrogen and these variables\n",
        "\n",
        "read the manuscript for further discussion\n",
        "\n",
        "# Import libraries and initialize google earth engine"
      ],
      "id": "a83b97ce-45e0-463b-9b7c-e79a3c949299"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import os \n",
        "import time\n",
        "\n",
        "# Initialize Earth Engine\n",
        "ee.Initialize()\n"
      ],
      "id": "fa0b44ae-a001-41b2-bfb0-fb4051d24448"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# set relative work directory"
      ],
      "id": "2ce7e087-41e7-44de-b4a4-cbc5620e9b62"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'/home/datascience/herbivore_nutrient_interactions'"
            ]
          }
        }
      ],
      "source": [
        "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
        "os.getcwd()"
      ],
      "id": "526f163d-a835-487f-b6e7-fc8ba9a75d6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Australia shape"
      ],
      "id": "822bb9ad-321c-41d3-863b-d6fe6bf53726"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the countries feature collection\n",
        "countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n",
        "\n",
        "# Filter the feature collection to get the boundary of Australia\n",
        "australia = countries.filter(ee.Filter.eq('country_na', 'Australia'))\n",
        "\n"
      ],
      "id": "5f3c01ac-f301-405e-ba0c-7b87cc1fa8c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load in rasters"
      ],
      "id": "6b1b1e04-1821-4fb5-99e9-79982bbc0e09"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the two rasters\n",
        "raster1 = ee.ImageCollection('CSIRO/SLGA') \\\n",
        "    .filter(ee.Filter.eq('attribute_code', 'NTO')) \\\n",
        "    .select(['NTO_000_005_EV', 'NTO_005_015_EV']).mean()\n",
        "\n",
        "raster2 = ee.ImageCollection(\"NASA/MEASURES/GFCC/TC/v3\") \\\n",
        "    .select(\"tree_canopy_cover\") \\\n",
        "    .filter(ee.Filter.calendarRange(2000, 2017, 'year')).mean()\n",
        "\n",
        "raster3 = ee.Image(\"WORLDCLIM/V1/BIO\").select('bio12')\n",
        "\n",
        "raster4 = ee.ImageCollection('CSIRO/SLGA') \\\n",
        "    .filter(ee.Filter.eq('attribute_code', 'PTO')) \\\n",
        "    .select(['PTO_000_005_EV', 'PTO_005_015_EV']).mean()\n",
        "\n",
        "# Combine the temperature bands from each image into a single image\n",
        "temp_bands = raster1.addBands(raster2).addBands(raster3).addBands(raster4)\n",
        "\n"
      ],
      "id": "6e574186-71f8-474b-ad3b-561701b536b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sample the australia shapefile"
      ],
      "id": "f8c9db3e-5a66-400f-8854-9e1b60ef6c97"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Generate random points\n",
        "sample = ee.FeatureCollection.randomPoints(\n",
        "        region=australia, points=100000, seed=420, maxError=1\n",
        ")\n",
        "\n",
        "# Sample the the bands using the sample point feature collection.\n",
        "imgSamp = temp_bands.sampleRegions(\n",
        "  collection = sample,\n",
        "  scale = 30\n",
        ")\n"
      ],
      "id": "90f8c383-453f-41ee-844e-afaf21a984d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For loop to extract data from google earth engine"
      ],
      "id": "5a0321f3-7ee2-40a6-a80e-1c09dcb4bc39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 to 500\n",
            "Processed batch 501 to 1000\n",
            "Processed batch 1001 to 1500\n",
            "Processed batch 1501 to 2000\n",
            "Processed batch 2001 to 2500\n",
            "Processed batch 2501 to 3000\n",
            "Processed batch 3001 to 3500\n",
            "Processed batch 3501 to 4000\n",
            "Processed batch 4001 to 4500\n",
            "Processed batch 4501 to 5000\n",
            "Processed batch 5001 to 5500\n",
            "Processed batch 5501 to 6000\n",
            "Processed batch 6001 to 6500\n",
            "Processed batch 6501 to 7000\n",
            "Processed batch 7001 to 7500\n",
            "Processed batch 7501 to 8000\n",
            "Processed batch 8001 to 8500\n",
            "Processed batch 8501 to 9000\n",
            "Processed batch 9001 to 9500\n",
            "Processed batch 9501 to 10000\n",
            "Processed batch 10001 to 10500\n",
            "Processed batch 10501 to 11000\n",
            "Processed batch 11001 to 11500\n",
            "Processed batch 11501 to 12000\n",
            "Processed batch 12001 to 12500\n",
            "Processed batch 12501 to 13000\n",
            "Processed batch 13001 to 13500\n",
            "Processed batch 13501 to 14000\n",
            "Processed batch 14001 to 14500\n",
            "Processed batch 14501 to 15000\n",
            "Processed batch 15001 to 15500\n",
            "Processed batch 15501 to 16000\n",
            "Processed batch 16001 to 16500\n",
            "Processed batch 16501 to 17000\n",
            "Processed batch 17001 to 17500\n",
            "Processed batch 17501 to 18000\n",
            "Processed batch 18001 to 18500\n",
            "Processed batch 18501 to 19000\n",
            "Processed batch 19001 to 19500\n",
            "Processed batch 19501 to 20000\n",
            "Processed batch 20001 to 20500\n",
            "Processed batch 20501 to 21000\n",
            "Processed batch 21001 to 21500\n",
            "Processed batch 21501 to 22000\n",
            "Processed batch 22001 to 22500\n",
            "Processed batch 22501 to 23000\n",
            "Processed batch 23001 to 23500\n",
            "Processed batch 23501 to 24000\n",
            "Processed batch 24001 to 24500\n",
            "Processed batch 24501 to 25000\n",
            "Processed batch 25001 to 25500\n",
            "Processed batch 25501 to 26000\n",
            "Processed batch 26001 to 26500\n",
            "Processed batch 26501 to 27000\n",
            "Processed batch 27001 to 27500\n",
            "Processed batch 27501 to 28000\n",
            "Processed batch 28001 to 28500\n",
            "Processed batch 28501 to 29000\n",
            "Processed batch 29001 to 29500\n",
            "Processed batch 29501 to 30000\n",
            "Processed batch 30001 to 30500\n",
            "Processed batch 30501 to 31000\n",
            "Processed batch 31001 to 31500\n",
            "Processed batch 31501 to 32000\n",
            "Processed batch 32001 to 32500\n",
            "Processed batch 32501 to 33000\n",
            "Processed batch 33001 to 33500\n",
            "Processed batch 33501 to 34000\n",
            "Processed batch 34001 to 34500\n",
            "Processed batch 34501 to 35000\n",
            "Processed batch 35001 to 35500\n",
            "Processed batch 35501 to 36000\n",
            "Processed batch 36001 to 36500\n",
            "Processed batch 36501 to 37000\n",
            "Processed batch 37001 to 37500\n",
            "Processed batch 37501 to 38000\n",
            "Processed batch 38001 to 38500\n",
            "Processed batch 38501 to 39000\n",
            "Processed batch 39001 to 39500\n",
            "Processed batch 39501 to 40000\n",
            "Processed batch 40001 to 40500\n",
            "Processed batch 40501 to 41000\n",
            "Processed batch 41001 to 41500\n",
            "Processed batch 41501 to 42000\n",
            "Processed batch 42001 to 42500\n",
            "Processed batch 42501 to 43000\n",
            "Processed batch 43001 to 43500\n",
            "Processed batch 43501 to 44000\n",
            "Processed batch 44001 to 44500\n",
            "Processed batch 44501 to 45000\n",
            "Processed batch 45001 to 45500\n",
            "Processed batch 45501 to 46000\n",
            "Processed batch 46001 to 46500\n",
            "Processed batch 46501 to 47000\n",
            "Processed batch 47001 to 47500\n",
            "Processed batch 47501 to 48000\n",
            "Processed batch 48001 to 48500\n",
            "Processed batch 48501 to 49000\n",
            "Processed batch 49001 to 49500\n",
            "Processed batch 49501 to 50000"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set batch size\n",
        "batch_size = 500\n",
        "total_points = 100000\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Process in batches of 500\n",
        "for start in range(0, total_points, batch_size):\n",
        "    # Define the end of the batch\n",
        "    end = min(start + batch_size, total_points)\n",
        "\n",
        "    # Get the batch of points\n",
        "    batch_sample = sample.toList(batch_size, start)\n",
        "    batch_sample_fc = ee.FeatureCollection(batch_sample)\n",
        "\n",
        "    # Sample the raster data\n",
        "    img_samp_batch = temp_bands.sampleRegions(\n",
        "        collection=batch_sample_fc,\n",
        "        scale=30\n",
        "    )\n",
        "\n",
        "    # Get the result as a list of dictionaries\n",
        "    try:\n",
        "        sample_dict = img_samp_batch.getInfo()['features']\n",
        "        rows = [feature['properties'] for feature in sample_dict]\n",
        "        df_batch = pd.DataFrame(rows)\n",
        "\n",
        "        # Append the batch to the main DataFrame\n",
        "        df = pd.concat([df, df_batch], ignore_index=True)\n",
        "\n",
        "        print(f\"Processed batch {start + 1} to {end}\")\n",
        "\n",
        "        # Add a delay to avoid overwhelming the API\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch {start + 1} to {end}: {e}\")\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "id": "075e31d3-8462-4648-8b70-41e6c5756786"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('data/processed/spatial_modeling/environmental_correlation_data.csv', index=False)\n"
      ],
      "id": "0572afc1-3e41-4dd5-87d8-bf9bdc360b75"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}