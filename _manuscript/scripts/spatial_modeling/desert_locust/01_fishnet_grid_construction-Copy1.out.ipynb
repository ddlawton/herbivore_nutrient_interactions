{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Point to grid conversion\n",
        "\n",
        "This code takes the georeferenced points and related them to a fishnet\n",
        "grid of 1km x 1km scale.\n",
        "\n",
        "This will be for both for Australian Plague Locust and Desert Locust"
      ],
      "id": "5cfe6ea6-24d3-431d-a91c-a658d28729e9"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import os"
      ],
      "id": "8cb97841-63d4-4070-a986-089de1d711e8"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'/home/datascience/cleaned_aplc_soils_project'"
            ]
          }
        }
      ],
      "source": [
        "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
        "os.getcwd()"
      ],
      "id": "1184a69a-588f-4522-bd5a-04540769e463"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function takes the point dataset and creates a sparse fishnet grid that overlaps with all points. \n",
        "# there is a user species cell size and few other handy parameters\n",
        "# it defaults to parallel processing but can be turned off\n",
        "\n",
        "# it creates a bounding box, creates a coarse grid, filters out unused grids and then within each coarse grid, it creates the fine scale grid\n",
        "# at the speicifed cell size and then removes unused grids.\n",
        "\n",
        "with open('../cleaned_aplc_soils_project/scripts/functions/fishnet_grid_function.py') as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Execute the code\n",
        "exec(code)\n"
      ],
      "id": "20638780-1e5a-4d63-b733-5b41a2670c7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Australian plague locust data\n",
        "\n",
        "This dataset is comprised of several species but the records are biased\n",
        "towards the more economically important species (e.g. australian plauge\n",
        "locust)\n",
        "\n",
        "## Species identifier\n",
        "\n",
        "The `Species` column is the identifier per species\n",
        "\n",
        "-   10 = no locust/grasshopper found\n",
        "-   11 = Australian plague locust (*Chortocietes terminifera*)\n",
        "-   12 = Spur-throated locust (*Austracris guttulosa*)\n",
        "-   15 = Small plague locust (*Austroicetes cruciata*)\n",
        "-   19 = Eastern plague grasshopper (*Oedaleus australis*)\n",
        "\n",
        "### Species data management\n",
        "\n",
        "For each species, we need to combined the species specific ID with the\n",
        "`10` observations (nil-observations) and observations in the other\n",
        "species categories that are nil. This will provide us with all the\n",
        "records that a specific grasshopper species was present and combine it\n",
        "with all the nil observation possible."
      ],
      "id": "4b55f198-1b63-40cb-900e-d016ab77e533"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../cleaned_aplc_soils_project/data/raw/survey_data/APLC_surveys_2000-2017.csv')"
      ],
      "id": "19398ba1-1ad5-44d4-a42c-c26ee37c0a52"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_and_update_species(df, species_list, target_species):\n",
        "    and_nil = df['Species'].isin(species_list)\n",
        "    all_other_nil = (\n",
        "        ~df['Species'].isin(species_list) &\n",
        "        (df['Nymph Density'] == 0) &\n",
        "        (df['Adult Density'] == 0)\n",
        "    )\n",
        "    filtered_df = df[and_nil | all_other_nil].copy()  # Use copy() to avoid SettingWithCopyWarning\n",
        "    filtered_df['Species'] = target_species\n",
        "    return filtered_df\n",
        "\n",
        "# Define the list of target species and corresponding species lists\n",
        "species_combinations = [\n",
        "    ([10, 11], 11),\n",
        "    ([10, 12], 12),\n",
        "    ([10, 15], 15),\n",
        "    ([10, 19], 19)\n",
        "]\n",
        "\n",
        "# Use a list comprehension to generate all DataFrames in one step\n",
        "full_species_data = pd.concat(\n",
        "    [filter_and_update_species(df, species_list, target_species) \n",
        "     for species_list, target_species in species_combinations], \n",
        "    ignore_index=True\n",
        ").drop_duplicates()  # Optionally drop duplicates\n",
        "\n",
        "# final_df now contains the combined result\n",
        "full_species_data.head()"
      ],
      "id": "f22158aa-4a9b-4b5f-a743-c92efb33e85e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fishnet grid construction\n",
        "\n",
        "I first create the sparse fishnet grid (roughly 1km x 1km) and then\n",
        "aggregate the species data to the polygons. This is done BY species and\n",
        "not between."
      ],
      "id": "8ac76f59-072e-401e-ad14-832fcce91ee8"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Polygons: 100%|██████████| 1315/1315 [00:14<00:00, 88.39it/s] "
          ]
        }
      ],
      "source": [
        "\n",
        "aplc_survey_dataframe = create_grids_parallel(\n",
        "        full_species_data,\n",
        "        longitude_column='Longitude',\n",
        "        latitude_column='Latitude',\n",
        "        csv_crs='EPSG:4326',\n",
        "        grid_crs='EPSG:4326',\n",
        "        transformation_crs='EPSG:4326',\n",
        "        final_cell_size=0.01,\n",
        "        coarse_cell_size=None,\n",
        "        parallel=True\n",
        "    )\n",
        "\n"
      ],
      "id": "b518e127-c4da-4ed5-a189-161cb686a630"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>67176 rows × 2 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Add an index to the polygons\n",
        "## this will allow us to join tables together (and make GEE easier)\n",
        "\n",
        "aplc_survey_dataframe = gpd.GeoDataFrame(aplc_survey_dataframe)\n",
        "aplc_survey_dataframe['polygon_id'] = aplc_survey_dataframe.index\n",
        "aplc_survey_dataframe.set_crs(epsg=4326, inplace=True)"
      ],
      "id": "cd8ea9d3-c931-4001-81c5-983391ccfa35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lets summarize the APL point data to the polygon grid data"
      ],
      "id": "0cc780c0-d44b-4af8-b1ef-52919ccfc6de"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_keep = ['Longitude', 'Latitude', 'Date','Species','Data Quality','Nymph Density','Adult Density']\n",
        "APL_dat = final_df[columns_to_keep]\n"
      ],
      "id": "674d1a74-aa45-4e26-a3b1-335111a9e054"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>309640 rows × 8 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Convert DataFrame to GeoDataFrame\n",
        "\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    APL_dat,\n",
        "    geometry=gpd.points_from_xy(APL_dat.Longitude, APL_dat.Latitude)\n",
        ")\n",
        "\n",
        "gdf_points.set_crs(epsg=4326, inplace=True)\n"
      ],
      "id": "b13906f9-6919-4226-ad4b-d9788b6036c3"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "joined_gdf = gpd.sjoin(gdf_points, aplc_survey_dataframe, how=\"left\", predicate='within')\n"
      ],
      "id": "b979461e-3cec-4b0d-8fe6-2b6a058a46b6"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitude        0\n",
            "Latitude         0\n",
            "Date             0\n",
            "Species          0\n",
            "Data Quality     0\n",
            "Nymph Density    0\n",
            "Adult Density    0\n",
            "geometry         0\n",
            "index_right      0\n",
            "polygon_id       0\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "# I wanted to ensure every point has an assoicated polygon_id \n",
        "\n",
        "## which they do\n",
        "\n",
        "na_counts = joined_gdf.isna().sum()\n",
        "\n",
        "print(na_counts)"
      ],
      "id": "4982db06-9b18-401d-99a2-8e4fb78a5a80"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom aggregation function to get min, median, and max dates\n",
        "def date_aggregations(series):\n",
        "    series = pd.to_datetime(series, format='%d-%b-%y') # converts to a datatime\n",
        "    return [series.min(), series.median(), series.max()]\n",
        "\n",
        "# aggregation function to count unique values\n",
        "def count_unique_values(series):\n",
        "    return series.value_counts().to_dict()\n",
        "\n",
        "# Group by polygon geometry and aggregate the data\n",
        "aggregated_gdf = joined_gdf.groupby(['Species', 'polygon_id']).agg({\n",
        "    'Date': date_aggregations,\n",
        "    'Longitude': 'median',\n",
        "    'Latitude': 'median',\n",
        "    'Data Quality': count_unique_values,\n",
        "    'Nymph Density': count_unique_values,\n",
        "    'Adult Density': count_unique_values\n",
        "}).reset_index()\n"
      ],
      "id": "154aac4f-b185-43d6-9897-22eec2641829"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>177613 rows × 8 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "aggregated_gdf"
      ],
      "id": "a1aecc29-106a-4d7b-bd83-c06e07c84cc5"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Lets count up the total number of observations in the dataframe:\n",
        "\n",
        "# Define the aggregation functions for each column\n",
        "agg_funcs = {\n",
        "    'Data Quality': 'count',  \n",
        "    'Adult Density': 'count',    \n",
        "    'Nymph Density': 'count'    \n",
        "}\n",
        "\n",
        "# Perform groupby operation and aggregate\n",
        "grouped_joined_gdf = joined_gdf.groupby(['Species', 'polygon_id']).agg(agg_funcs).reset_index()\n",
        "\n",
        "# Rename the aggregated columns\n",
        "grouped_joined_gdf.rename(columns={\n",
        "    'Data Quality': 'Data Quality Total Count',\n",
        "    'Adult Density': 'Adult Density Total Count',\n",
        "    'Nymph Density': 'Nymph Density Total Count'\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "# These three columns should agree with one another....\n",
        "## if any rows dont -- this command filters for them\n",
        "grouped_joined_gdf[\n",
        "    (grouped_joined_gdf['Data Quality Total Count'] != grouped_joined_gdf['Adult Density Total Count']) |\n",
        "    (grouped_joined_gdf['Data Quality Total Count'] != grouped_joined_gdf['Nymph Density Total Count']) |\n",
        "    (grouped_joined_gdf['Adult Density Total Count'] != grouped_joined_gdf['Nymph Density Total Count'])\n",
        "]"
      ],
      "id": "220eaf11-46b0-46e3-a435-af9fcf1fdefc"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Species                      0\n",
            "polygon_id                   0\n",
            "Date                         0\n",
            "Longitude                    0\n",
            "Latitude                     0\n",
            "Data Quality                 0\n",
            "Nymph Density                0\n",
            "Adult Density                0\n",
            "Data Quality Total Count     0\n",
            "Adult Density Total Count    0\n",
            "Nymph Density Total Count    0\n",
            "geometry                     0\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "final_df = pd.merge(aggregated_gdf, grouped_joined_gdf, on=['Species', 'polygon_id'], how='left')\n",
        "final_df = pd.merge(final_df, aplc_survey_dataframe, on='polygon_id', how='left')\n",
        "final_df.head()\n",
        "na_counts = final_df.isna().sum()\n",
        "\n",
        "print(na_counts)"
      ],
      "id": "66a01f23-45b0-41f3-8efa-c2f0244c59fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The APLC data is complete – lets write to disk"
      ],
      "id": "95437c8a-ba1b-49a8-8522-ed6bfc5f9280"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv('../cleaned_aplc_soils_project/data/processed/spatial_modeling/aplc_data_aggregated_to_polygon_grid.csv')"
      ],
      "id": "53baf12a-4255-4c56-997e-b89028f78bac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desert Locust dataset\n",
        "\n",
        "now there are two public desert locust datasets: ‘hoppers’ and ‘bands’\n",
        "\n",
        "I’m not going to do too much with this dataset – I just want to get the\n",
        "polygon grids constructed and potentially return to it if we need to"
      ],
      "id": "91786a49-66b0-479b-903d-714945c4ac4a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "bands_df = pd.read_csv('cleaned_aplc_soils_project/data/raw/survey_data/desert_locust/Bands_Public_3599395965147926909.csv')\n",
        "hoppers_df = pd.read_csv('cleaned_aplc_soils_project/data/raw/survey_data/desert_locust/Hoppers_Public_7481916456258955251.csv')\n",
        "swarms_df = pd.read_csv('cleaned_aplc_soils_project/data/raw/survey_data/desert_locust/Swarms_Public_891175385870988899.csv')\n",
        "adults_df = pd.read_csv('cleaned_aplc_soils_project/data/raw/survey_data/desert_locust/Adults_Public_-3113927055772371667.csv')\n",
        "\n",
        "\n",
        "columns_to_keep = ['STARTDATE', 'LOCPRESENT', 'LOCRELIAB','REPRELIAB', 'SHPDENISOL', 'SHPDENSCAT', \n",
        "                       'SHPDENGRP', 'SHPDENUNK', 'GHPDENLOW', 'GHPDENMED', 'GHPDENHI', \n",
        "                       'GHPDENUNK', 'SADDENISOL', 'SADDENSCAT', 'SADDENGRP', 'SADDENUNK',\n",
        "                       'GADDENLOW', 'GADDENMED', 'GADDENHI', 'GADDENUNK', 'CAT', 'x', 'y']\n",
        "# Check if columns in all DataFrames match\n",
        "\n",
        "# Check if columns in all DataFrames match\n",
        "if bands_df.columns.equals(hoppers_df.columns) and \\\n",
        "   bands_df.columns.equals(swarms_df.columns) and \\\n",
        "   bands_df.columns.equals(adults_df.columns):\n",
        "    \n",
        "    # Columns match, concatenate the DataFrames\n",
        "    combined_dl_df = pd.concat([bands_df, hoppers_df, swarms_df, adults_df], ignore_index=True)\n",
        "    \n",
        "    # Filter for specified columns\n",
        "    combined_dl_df = combined_dl_df[columns_to_keep]\n",
        "else:\n",
        "    # Columns don't match, raise an error\n",
        "    raise ValueError(\"Columns of the DataFrames do not match.\")"
      ],
      "id": "7fc2b839-8828-4101-808e-f55b9e2331ae"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>250705 rows × 23 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "combined_dl_df"
      ],
      "id": "2798fe93-80eb-462e-90bb-34e496c11954"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map specific columns to desired values\n",
        "column_mapping = {\n",
        "    'LOCPRESENT': {1: 'present', 2: 'absent'},\n",
        "    'REPRELIAB': {0: 'dubious', 1: 'reliable'},\n",
        "    'SHPDENISOL': {0: 'no', 1: 'isolated'},\n",
        "    'SHPDENSCAT': {0: 'no', 1: 'scattered'},\n",
        "    'SHPDENGRP': {0: 'no', 1: 'group'},\n",
        "    'SHPDENUNK': {0: 'no', 1: 'unkown'},\n",
        "    'GHPDENLOW': {0: 'no', 1: 'low'},\n",
        "    'GHPDENMED': {0: 'no', 1: 'medium'},\n",
        "    'GHPDENHI': {0: 'no', 1: 'high'},\n",
        "    'GHPDENUNK': {0: 'no', 1: 'unkown'},\n",
        "    'SADDENISOL': {0: 'no', 1: 'isolated'},\n",
        "    'SADDENSCAT': {0: 'no', 1: 'scattered'},\n",
        "    'SADDENGRP': {0: 'no', 1: 'group'},\n",
        "    'SADDENUNK': {0: 'no', 1: 'unkown'},\n",
        "    'GADDENLOW': {0: 'no', 1: 'low'},\n",
        "    'GADDENMED': {0: 'no', 1: 'medium'},\n",
        "    'GADDENHI': {0: 'no', 1: 'high'},\n",
        "    'GADDENUNK': {0: 'no', 1: 'unknown'}\n",
        "}\n",
        "\n",
        "for column, mapping in column_mapping.items():\n",
        "    combined_dl_df[column] = combined_dl_df[column].map(mapping)"
      ],
      "id": "272a3c30-19be-4883-b817-bf16fce66104"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>250705 rows × 11 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Assuming combined_dl_df is your DataFrame containing the columns\n",
        "\n",
        "# Define column groups\n",
        "column_groups = {\n",
        "    'SHP': ['SHPDENISOL', 'SHPDENSCAT', 'SHPDENGRP', 'SHPDENUNK'],\n",
        "    'GHP': ['GHPDENLOW', 'GHPDENMED', 'GHPDENHI', 'GHPDENUNK'],\n",
        "    'SAD': ['SADDENISOL', 'SADDENSCAT', 'SADDENGRP', 'SADDENUNK'],\n",
        "    'GAD': ['GADDENLOW', 'GADDENMED', 'GADDENHI', 'GADDENUNK']\n",
        "}\n",
        "\n",
        "# Create a new DataFrame to store collapsed values\n",
        "collapsed_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each group\n",
        "for group_prefix, columns in column_groups.items():\n",
        "    # Select columns belonging to the current group\n",
        "    group_data = combined_dl_df[columns]\n",
        "    # Determine collapsed value for each row in the group\n",
        "    collapsed_value = group_data.apply(lambda row: '_'.join(row[row != 'no']) if sum(row != 'no') > 1 else row[row != 'no'].values[0] if sum(row != 'no') == 1 else 'no', axis=1)\n",
        "    # Assign collapsed value to a new column in the collapsed_df\n",
        "    collapsed_df[group_prefix] = collapsed_value\n",
        "\n",
        "# Drop the original columns that were collapsed from the original DataFrame\n",
        "collapsed_columns = [column for columns in column_groups.values() for column in columns]\n",
        "original_columns = [column for column in combined_dl_df.columns if column not in collapsed_columns]\n",
        "original_df = combined_dl_df[original_columns]\n",
        "\n",
        "# Concatenate the original DataFrame with the collapsed DataFrame\n",
        "final_df = pd.concat([original_df, collapsed_df], axis=1)\n",
        "\n",
        "# Display the final DataFrame\n",
        "final_df"
      ],
      "id": "02fa8c7e-544d-4eeb-a63b-c324b4f90ea1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So there is a lot of hand-waviness when it comes to this data.\n",
        "`LOCPRESENT` does not necessarily agree with the `SHP`, `GHP`, `SAD`, or\n",
        "`GAD` columns.\n",
        "\n",
        "I think for I will take only into consideration the SHP, GHP, SAD, and\n",
        "GAD columns and loosely categorize them into broad catorgies of dense\n",
        "vs. not.\n",
        "\n",
        "With this said, there are a bunch of overlaps….check out the counts\n",
        "table below."
      ],
      "id": "e021cf46-0128-4cd4-a6fa-89506fd82f99"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "SHP                       GHP            \n",
              "group                     high                 3160\n",
              "                          low                   600\n",
              "                          low_high               76\n",
              "                          low_medium             38\n",
              "                          low_medium_high         8\n",
              "                          medium               3863\n",
              "                          medium_high           395\n",
              "                          no                  35076\n",
              "                          unkown                113\n",
              "isolated                  high                    9\n",
              "                          low                    44\n",
              "                          medium                 26\n",
              "                          no                   8257\n",
              "isolated_group            medium                  9\n",
              "                          no                      3\n",
              "isolated_scattered        no                     46\n",
              "isolated_scattered_group  no                      2\n",
              "no                        high                 7613\n",
              "                          low                  2386\n",
              "                          low_high              134\n",
              "                          low_medium            164\n",
              "                          low_medium_high        56\n",
              "                          medium               8209\n",
              "                          medium_high           619\n",
              "                          no                 157355\n",
              "                          unkown               3279\n",
              "scattered                 high                  128\n",
              "                          low                   226\n",
              "                          low_medium              3\n",
              "                          medium                400\n",
              "                          medium_high            11\n",
              "                          no                  17319\n",
              "                          unkown                  8\n",
              "scattered_group           high                    5\n",
              "                          low                    18\n",
              "                          low_medium              9\n",
              "                          low_medium_high         3\n",
              "                          medium                 26\n",
              "                          no                    258\n",
              "                          unkown                  2\n",
              "unkown                    high                   25\n",
              "                          low                     2\n",
              "                          medium                 24\n",
              "                          medium_high             4\n",
              "                          no                    625\n",
              "                          unkown                 69\n",
              "Name: x, dtype: int64"
            ]
          }
        }
      ],
      "source": [
        "final_df.groupby(['SHP','GHP']).count()['x']"
      ],
      "id": "85c4c3c4-09ee-42c1-93a1-6b17493e9f97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I think we can debate for a while the correct way to handle this data.\n",
        "\n",
        "For now, lets just go forward and do the polygon gridding like we did\n",
        "with the APLC data."
      ],
      "id": "02bf3f01-d5f6-4b58-9d92-47eab053f602"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Polygons: 100%|██████████| 287/287 [02:21<00:00,  2.02it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "dl_survey_dataframe = create_grids_parallel(\n",
        "        final_df,\n",
        "        longitude_column='x',\n",
        "        latitude_column='y',\n",
        "        csv_crs='EPSG:4326',\n",
        "        grid_crs='EPSG:4326',\n",
        "        transformation_crs='EPSG:4326',\n",
        "        final_cell_size=0.01,\n",
        "        coarse_cell_size=None,\n",
        "        parallel=True\n",
        "    )"
      ],
      "id": "1faf65e1-de2a-4c33-84b0-fba4c6776cb6"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>130690 rows × 2 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Add an index to the polygons\n",
        "## this will allow us to join tables together (and make GEE easier)\n",
        "\n",
        "dl_survey_dataframe = gpd.GeoDataFrame(dl_survey_dataframe)\n",
        "dl_survey_dataframe['polygon_id'] = dl_survey_dataframe.index\n",
        "dl_survey_dataframe.set_crs(epsg=4326, inplace=True)"
      ],
      "id": "31c59989-d85d-4c56-9444-562fa863bced"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>250705 rows × 12 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Convert DataFrame to GeoDataFrame\n",
        "\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    final_df,\n",
        "    geometry=gpd.points_from_xy(final_df.x, final_df.y)\n",
        ")\n",
        "\n",
        "gdf_points.set_crs(epsg=4326, inplace=True)\n"
      ],
      "id": "1f597b45-5a68-40e1-8dab-958a0968f974"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "joined_gdf = gpd.sjoin(gdf_points, dl_survey_dataframe, how=\"left\", predicate='within')\n"
      ],
      "id": "642b3973-8c52-4608-b59e-7f61270deaef"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>251566 rows × 14 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "joined_gdf"
      ],
      "id": "2646c62b-f76d-4eea-a7d2-dd6ee40a74e9"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom aggregation function to get min, median, and max dates\n",
        "def date_aggregations(series):\n",
        "    series = pd.to_datetime(series, format='%m/%d/%Y %I:%M:%S %p') # converts to a datatime\n",
        "    return [series.min(), series.median(), series.max()]\n",
        "\n",
        "# aggregation function to count unique values\n",
        "def count_unique_values(series):\n",
        "    return series.value_counts().to_dict()\n",
        "\n",
        "# Group by polygon geometry and aggregate the data\n",
        "aggregated_gdf = joined_gdf.groupby(['polygon_id']).agg({\n",
        "    'STARTDATE': date_aggregations,\n",
        "    'x': 'median',\n",
        "    'y': 'median',\n",
        "    'LOCPRESENT': count_unique_values,\n",
        "    'REPRELIAB': count_unique_values,\n",
        "    'CAT': count_unique_values,\n",
        "    'SHP': count_unique_values,\n",
        "    'GHP': count_unique_values,\n",
        "    'SAD': count_unique_values,\n",
        "    'GAD': count_unique_values\n",
        "}).reset_index()\n"
      ],
      "id": "3d6b56dc-b9b0-428d-8efe-425c4132a8d2"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets count up the total number of observations in the dataframe:\n",
        "\n",
        "# Define the aggregation functions for each column\n",
        "agg_funcs = {\n",
        "    'CAT': 'count',\n",
        "    'SHP': 'count',\n",
        "    'GHP': 'count',\n",
        "    'SAD': 'count',\n",
        "    'GAD': 'count',\n",
        "    'LOCPRESENT': 'count',    \n",
        "    'REPRELIAB': 'count'    \n",
        "}\n",
        "\n",
        "# Perform groupby operation and aggregate\n",
        "grouped_joined_gdf = joined_gdf.groupby(['polygon_id']).agg(agg_funcs).reset_index()\n",
        "\n",
        "# Rename the aggregated columns\n",
        "grouped_joined_gdf.rename(columns={\n",
        "    'CAT': 'total_CAT_count',\n",
        "    'SHP': 'total_SHP_count',\n",
        "    'GHP': 'total_GHP_count',\n",
        "    'SAD': 'total_SAD_count',\n",
        "    'GAD': 'total_GAD_count',\n",
        "    'LOCPRESENT': 'total_LOCPRESENT_count',    \n",
        "    'REPRELIAB': 'total_REPRELIAB_count'    \n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "# These three columns should agree with one another....\n",
        "## if any rows dont -- this command filters for them\n",
        "\n",
        "grouped_joined_gdf['unique_counts'] = grouped_joined_gdf.apply(lambda row: len(set(row[1:])), axis=1)\n",
        "inconsistent_rows = grouped_joined_gdf[grouped_joined_gdf['unique_counts'] > 1].drop(columns=['unique_counts'])\n",
        "\n"
      ],
      "id": "c8cb7f92-a8c5-48c6-9bd7-f9fad378116b"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>540 rows × 8 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "inconsistent_rows"
      ],
      "id": "5b727b15-cd8d-49ed-b56d-e81045e2ca21"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the reliability column does agree – but thats it. I dont think its a\n",
        "big concern…"
      ],
      "id": "b72ef4e9-214b-4ac6-b887-015c72c6dcc3"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>130690 rows × 20 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "final_df = pd.merge(aggregated_gdf, grouped_joined_gdf, on=['polygon_id'], how='left')\n",
        "final_df = pd.merge(final_df, dl_survey_dataframe, on='polygon_id', how='left')\n",
        "final_df\n"
      ],
      "id": "8b9911cb-ccfa-463d-b1b3-c54459b35ce5"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polygon_id                0\n",
            "STARTDATE                 0\n",
            "x                         0\n",
            "y                         0\n",
            "LOCPRESENT                0\n",
            "REPRELIAB                 0\n",
            "CAT                       0\n",
            "SHP                       0\n",
            "GHP                       0\n",
            "SAD                       0\n",
            "GAD                       0\n",
            "total_CAT_count           0\n",
            "total_SHP_count           0\n",
            "total_GHP_count           0\n",
            "total_SAD_count           0\n",
            "total_GAD_count           0\n",
            "total_LOCPRESENT_count    0\n",
            "total_REPRELIAB_count     0\n",
            "unique_counts             0\n",
            "geometry                  0\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "na_counts = final_df.isna().sum()\n",
        "\n",
        "print(na_counts)"
      ],
      "id": "298b0723-6fdc-4259-8bdf-1eaaacc22de1"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv('cleaned_aplc_soils_project/data/processed/spatial_modeling//dl_data_aggregated_to_polygon_grid.csv')"
      ],
      "id": "648bba26-3a7f-4b9e-9cd6-bdf923cd03ed"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}